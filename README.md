# LLM & Prompt Vulnerabilities

### Finding and documentating vulnerabilities in Generative Models based on prompt-engineering


- Prompt In the Middle (PITM)?
  - Injecting prompt to access other's output (https://sharegpt.com/c/nrCPDzJ)

- Nested Prompt Attack (Need a better name :D)
  - While Providing nested prompts, the model ignores the initial instructions (https://sharegpt.com/c/BWyhgyN)
  
