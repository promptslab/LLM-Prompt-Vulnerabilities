<div align="center">
<img width="110px" src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Spectre_logo_with_text.svg/300px-Spectre_logo_with_text.svg.png">
<h1>LLM & Prompt Vulnerabilities</h1></div>
<!-- 
<h2 align="center">Promptify</h2> -->

<p align="center">
  <p align="center">Finding and documentating vulnerabilities in Generative Models based on prompt-engineering
</p>
</p>

 <h4 align="center">
  <a href="http://makeapullrequest.com">
    <img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square" alt="http://makeapullrequest.com" />
  </a>
  <a href="https://discord.gg/m88xfYMbK6">
    <img src="https://img.shields.io/badge/Discord-Community-orange" alt="Community" />
  </a>
</h4>



|      Name                | Description  | proof |
| :-------------------- | :----------: | :----------: |
| **Prompt In the Middle (PITM)?** | Injecting prompt to access other's output | [[Proof]](https://sharegpt.com/c/nrCPDzJ) |
| **Nested Prompt Attack (Need a better name :D)** | While Providing nested prompts, the model ignores the initial instructions | [[Proof]](https://sharegpt.com/c/BWyhgyN) |
